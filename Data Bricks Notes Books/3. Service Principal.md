In normal terms it is like creating a customer account and giving them access to a portal or data warehouse.
1, Register Azure Ad Application / Service Principal
2, Generate a secret? password for the application
3, Set Spark Config with App?client Id, Directory/ Tenant id & Secert
Assign Role Storage Blob Data Contributor to the Data lake 


SOP: Access ADLS Using Azure Service Principal (Databricks)
Purpose: Authenticate Databricks to Azure Data Lake Storage using a Service Principal for secure, automated, production‑grade access.
_________________________________________________________________________________________________________________________________________________________
1. Register Service Principal (Azure AD Application)
Steps:

Go to Azure Portal → Azure Active Directory

Select App Registrations

Click New Registration

Name the app (e.g., formula1.app)

Leave account type as Single Tenant

Click Register

Outputs captured:

Client ID

Tenant ID

SOP Action:  
Store these in notebook variables:

python
client_id = "<client-id>"
tenant_id = "<tenant-id>"

_________________________________________________________________________________________________________________________________________________________
2. Create Client Secret
Steps:

In the Service Principal → Certificates & Secrets

Click New Client Secret

Add description

Choose expiry (default 6 months)

Click Add

Copy Secret Value (NOT Secret ID)

SOP Action:  
Store in notebook:

python
client_secret = "<secret-value>"
Critical Notes:

Do NOT mix up Client ID and Tenant ID

Do NOT copy Secret ID

Secret Value is visible only once

_________________________________________________________________________________________________________________________________________________________
3. Configure Spark for Service Principal Authentication
Steps:  
Set Spark configs in Databricks:

```python

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", 
               "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")

spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", client_id)
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", client_secret)

spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", 
               f"https://login.microsoftonline.com/{tenant_id}/oauth2/token")
Replace:  
<storage-account> → your <ADLS account name>
```
_________________________________________________________________________________________________________________________________________________________
4. Assign RBAC Role to Service Principal
Steps:

Go to Storage Account → Access Control (IAM)

Click Add → Add role assignment

Choose role:

Storage Blob Data Contributor (full access)

or Storage Blob Data Reader (read‑only)

Assign access to: User, group, or Service Principal

Select your Service Principal

Click Review + Assign

Purpose:  
Grants the SP permission to access ADLS via RBAC.

_________________________________________________________________________________________________________________________________________________________
5. Test Access from Databricks
List files:

python
display(dbutils.fs.ls("abfss://<container>@<storage-account>.dfs.core.windows.net/"))
Read file:

python
df = spark.read.csv("abfss://<container>@<storage-account>.dfs.core.windows.net/circuits.csv",
                    header=True, inferSchema=True)
display(df)

_________________________________________________________________________________________________________________________________________________________
6. Validation Checklist
Before troubleshooting, confirm:

Item	Must Be
Client ID	Correctly copied
Tenant ID	Correctly copied
Secret Value	Not Secret ID
Spark configs	Correct storage account name
RBAC role	Assigned to SP
Token endpoint	Uses correct tenant ID
Cluster	Restarted after config changes

_________________________________________________________________________________________________________________________________________________________
7. When to Use Service Principals
Automated Databricks Jobs

CI/CD pipelines

Production ETL workloads

Secure, auditable access

Multi‑application architecture

Two‑Word Logic (Your Style)
Register SP  
Create Secret  
Configure Spark  
Assign Role  
Test Access
